{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: We chose the LoRA (Low-Rank Adaptation) technique for Parameter-Efficient Fine-Tuning (PEFT). LoRA reduces the number of trainable parameters by decomposing the weight updates into low-rank matrices. This approach is computationally efficient and allows fine-tuning large language models with limited resources.\n",
    "* Model: The model selected for this project is GPT-2, specifically configured for sequence classification. GPT-2 is a robust, pre-trained language model known for its versatility in various NLP tasks. By using the GPT-2 model, we leverage its pre-trained knowledge and adapt it to the specific task of classifying well-formed queries.\n",
    "* Evaluation approach: The evaluation approach involves using the Hugging Face Trainer's evaluate method. This method provides a comprehensive evaluation framework, including metrics computation. The primary metric for this project is accuracy, which measures the proportion of correctly classified queries.\n",
    "* Fine-tuning dataset: The dataset used for fine-tuning is the \"Google Query Wellformedness\" dataset. This dataset consists of queries annotated for well-formedness, providing a binary classification task. Each query is rated on a scale from 0 to 1, and for this project, ratings are converted to binary labels, with 1 indicating a well-formed query (rating > 0.5) and 0 indicating a poorly-formed query (rating â‰¤ 0.5). This dataset is suitable for adapting GPT-2 to understand and classify the well-formedness of queries accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U scikit-learn\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding  # Import data collator for padding\n",
    "from datasets import load_dataset  # Import function to load dataset\n",
    "from sklearn.metrics import accuracy_score  # Import accuracy score function from scikit-learn\n",
    "import numpy as np  # Import NumPy library for numerical operations\n",
    "import pandas as pd  # Import Pandas library for data manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for google-research-datasets/google_wellformed_query contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google-research-datasets/google_wellformed_query\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset splits to load\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "# Load the dataset for each split using the load_dataset function from the datasets library\n",
    "# The dataset used here is \"google-research-datasets/google_wellformed_query\"\n",
    "# It contains data for training, validation, and testing\n",
    "# The splits variable specifies which splits to load\n",
    "dataset = {split: load_dataset(\"google-research-datasets/google_wellformed_query\", split=split) for split in splits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example from train:\n",
      "{'rating': 0.20000000298023224, 'content': 'The European Union includes how many ?'}\n",
      "\n",
      "First example from validation:\n",
      "{'rating': 1.0, 'content': 'Who discovered x-rays in 1885 ?'}\n",
      "\n",
      "First example from test:\n",
      "{'rating': 0.4000000059604645, 'content': 'Interesting facts about Egypt ?'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each split in the dataset\n",
    "for split in dataset:\n",
    "    # Print a descriptive message indicating the split being processed\n",
    "    print(f\"First example from {split}:\")\n",
    "    # Print the first example from the current split\n",
    "    print(dataset[split][0])\n",
    "    # Print an empty line for clarity between examples\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_statistics(dataset, subset_name):\n",
    "    \"\"\"\n",
    "    Retrieves and prints statistics for a specified segment of a dataset.\n",
    "    \n",
    "    Args:\n",
    "    dataset (Dataset): The dataset object which contains subsets like 'train', 'validation', or 'test'.\n",
    "    subset_name (str): The key for the subset to analyze, e.g., 'train'.\n",
    "\n",
    "    Outputs:\n",
    "    Prints statistics about the number of queries, the length of queries, and the distribution of ratings.\n",
    "    \"\"\"\n",
    "    # Access the specified subset of the dataset\n",
    "    subset = dataset[subset_name]\n",
    "\n",
    "    # Print the total number of queries in the subset\n",
    "    print(f\"Total queries in {subset_name} subset:\", subset.num_rows)\n",
    "    \n",
    "    # Determine and display the longest and shortest query\n",
    "    max_query_length = max(len(query['content']) for query in subset)\n",
    "    min_query_length = min(len(query['content']) for query in subset)\n",
    "    print(f\"Longest query in {subset_name} subset has {max_query_length} characters\")\n",
    "    print(f\"Shortest query in {subset_name} subset has {min_query_length} characters\")\n",
    "    \n",
    "    # Identify and display unique rating values\n",
    "    unique_ratings = set(subset['rating'])\n",
    "    print(f\"Unique ratings in {subset_name} subset:\", unique_ratings)\n",
    "    \n",
    "    # Calculate and display the frequency and percentage distribution of ratings\n",
    "    print(\"Rating distribution in the subset:\")\n",
    "    rating_frequencies = {rating: sum(1 for item in subset['rating'] if item == rating) for rating in unique_ratings}\n",
    "    total_queries = subset.num_rows\n",
    "    rating_percentages = {rating: (count / total_queries * 100) for rating, count in rating_frequencies.items()}\n",
    "    \n",
    "    for rating, percentage in rating_percentages.items():\n",
    "        print(f\"- Rating {rating}: {round(percentage, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "503be982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries in train subset: 17500\n",
      "Longest query in train subset has 200 characters\n",
      "Shortest query in train subset has 10 characters\n",
      "Unique ratings in train subset: {0.20000000298023224, 0.4000000059604645, 0.6000000238418579, 1.0, 0.800000011920929, 0.0, 0.1666666716337204, 0.8333333134651184, 0.6666666865348816, 0.5, 0.3333333432674408}\n",
      "Rating distribution in the subset:\n",
      "- Rating 0.20000000298023224: 15.88%\n",
      "- Rating 0.4000000059604645: 11.89%\n",
      "- Rating 0.6000000238418579: 11.44%\n",
      "- Rating 1.0: 23.94%\n",
      "- Rating 0.800000011920929: 14.73%\n",
      "- Rating 0.0: 21.56%\n",
      "- Rating 0.1666666716337204: 0.11%\n",
      "- Rating 0.8333333134651184: 0.18%\n",
      "- Rating 0.6666666865348816: 0.13%\n",
      "- Rating 0.5: 0.07%\n",
      "- Rating 0.3333333432674408: 0.07%\n"
     ]
    }
   ],
   "source": [
    "# obtain statstics for train subset\n",
    "analyze_query_statistics(dataset=dataset, subset_name='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d3cdf5",
   "metadata": {},
   "source": [
    "### Load Tokenizer and tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ee37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6a3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set EOS (end of sentence) TOKEN as PAD TOKEN\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74743d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(entries):\n",
    "    \"\"\"\n",
    "    Tokenizes text data from a dataset using a specified tokenizer.\n",
    "    This function adjusts the length of each text entry by truncating longer texts\n",
    "    and padding shorter ones to a uniform length, ensuring all sequences are of the same length.\n",
    "\n",
    "    Args:\n",
    "    entries (dict): A dictionary where the text data is stored under the 'content' key.\n",
    "\n",
    "    Returns:\n",
    "    dict: Contains the tokenized text with padding and truncation applied.\n",
    "    \"\"\"\n",
    "    # Tokenize the text data, ensuring all sequences are of the same length\n",
    "    return tokenizer(entries['content'], padding='max_length', truncation=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "022cf8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497a959c57174e2fb41923381bbb574a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac5cdd8fa574a769d392510d2bccb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62525589f023419287ccfaa3d37f555a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming 'dataset' is a dictionary of datasets for each split and 'splits' is a list of these split names\n",
    "tokenized_datasets = {}\n",
    "for segment in splits:\n",
    "    # Tokenize each part of the dataset using the defined preprocessing function.\n",
    "    # The 'map' function applies this preprocessing in batches for improved performance.\n",
    "    tokenized_datasets[segment] = dataset[segment].map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8ccdc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The European Union includes how many ?\n",
      "Tokenized input IDs:\n",
      "[464, 3427, 4479, 3407, 703, 867, 5633, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
     ]
    }
   ],
   "source": [
    "# Check if the tokenized data contains the 'content' key and adapt accordingly\n",
    "if 'content' in tokenized_datasets['train'][0]:\n",
    "    print(\"Original text:\")\n",
    "    print(tokenized_datasets['train'][0]['content'])\n",
    "\n",
    "# Always available after tokenization as this is what the tokenizer produces\n",
    "print(\"Tokenized input IDs:\")\n",
    "print(tokenized_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00c05876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text from validation set:\n",
      "Who discovered x-rays in 1885 ?\n",
      "Tokenized input IDs from validation set:\n",
      "[8241, 5071, 2124, 12, 20477, 287, 1248, 5332, 5633, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
     ]
    }
   ],
   "source": [
    "# Ensure you're using the correct variable and key to access the data\n",
    "if 'content' in tokenized_datasets['validation'][0]:\n",
    "    print(\"Original text from validation set:\")\n",
    "    print(tokenized_datasets['validation'][0]['content'])\n",
    "\n",
    "print(\"Tokenized input IDs from validation set:\")\n",
    "print(tokenized_datasets['validation'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11ca7a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9ebe7b3ff841feb9fcc20082da9115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0901729b15a8432fbe2c92ef6dc7e64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49956561889145f282f7dbad594c8764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_binary_labels(example):\n",
    "    \"\"\"\n",
    "    Adds binary labels to examples based on a rating threshold.\n",
    "\n",
    "    Args:\n",
    "    example (dict): A dictionary containing the example data, including the 'rating'.\n",
    "\n",
    "    Returns:\n",
    "    dict: The example with an additional 'labels' field indicating the binary label.\n",
    "    \"\"\"\n",
    "    # Assuming the rating is under 'rating' key\n",
    "    example['labels'] = 1 if example['rating'] > 0.5 else 0\n",
    "    return example\n",
    "\n",
    "# Apply this transformation to each split in your dataset\n",
    "dataset = {split: ds.map(add_binary_labels) for split, ds in dataset.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d0136d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load GPT-2 pre-trained model configured for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('gpt2', num_labels=2,\n",
    "                                                           id2label={0: 'NEGATIVE', 1: 'POSITIVE'},\n",
    "                                                           label2id={'NEGATIVE': 0, 'POSITIVE': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb7fb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the EOS token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d47a8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's pad token id to match the tokenizer's pad token id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99271328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc18e3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cb794e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the base model for our specific task (sentiment analysis in this case).\n",
    "model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8068fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of a model's predictions. This function is intended to be used as a metric for the Hugging Face Trainer.\n",
    "\n",
    "    Args:\n",
    "    eval_pred (tuple): A tuple containing model predictions and true labels. Predictions are typically provided as logits.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the computed mean accuracy of the model.\n",
    "    \"\"\"\n",
    "    # Unpack the tuple into predictions and actual labels\n",
    "    logits, true_labels = eval_pred\n",
    "\n",
    "    # Convert logits to predicted class labels\n",
    "    predicted_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Compute the accuracy: proportion of correct predictions\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "\n",
    "    # Return the accuracy as a dictionary\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5df7a586-9e22-4271-9b20-193f76a4bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd928aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer with the specified configuration\n",
    "pretrain_trainer = Trainer(\n",
    "    model=model,  # The pre-trained model to be used for training\n",
    "    args=training_args,  # Training arguments like learning rate, batch size, etc.\n",
    "    train_dataset=tokenized_datasets['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets['validation'],  # Evaluation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer for tokenizing inputs\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Data collator for padding sequences\n",
    "    compute_metrics=calculate_accuracy,  # Function to compute evaluation metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fec26a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [235/235 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results before fine-tuning: {'eval_loss': 1.6033490896224976, 'eval_accuracy': 0.5058666666666667, 'eval_runtime': 22.4814, 'eval_samples_per_second': 166.804, 'eval_steps_per_second': 10.453}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set before fine-tuning\n",
    "pretrain_results = pretrain_trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results before fine-tuning\n",
    "print(\"Evaluation results before fine-tuning:\", pretrain_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "696e8bc0-a6f2-481c-8143-58991f9e1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (2.2.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (4.39.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (0.30.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from peft) (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mniti\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mniti\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b437805-e76a-472e-93b6-6ad2db43976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PEFT Config for LoRA\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0909a0ad-b16d-4ca3-a8ff-b3c67b5efaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=['c_attn', 'c_proj'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d49dd5b-a6ee-4e40-be4b-77d97d8129dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 812,544 || all params: 125,253,888 || trainable%: 0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "028f82da-5cbb-4378-9fea-48858a5b3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments specifying various parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_model_output\",  # Directory to save model outputs\n",
    "    learning_rate=2e-5,  # Learning rate for optimization\n",
    "    per_device_train_batch_size=32,  # Batch size for training per device\n",
    "    per_device_eval_batch_size=32,  # Batch size for evaluation per device\n",
    "    num_train_epochs=10,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    evaluation_strategy=\"epoch\",  # Evaluation is performed at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Model is saved at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    logging_dir='./logs',  # Directory for logging metrics and/or losses during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c06a34fb-bac2-4dbd-8fc6-9b55ea781489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer for training the model\n",
    "trainer = Trainer(\n",
    "    model=peft_model,  # The model to be trained\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # Evaluation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer for encoding the data\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Data collator for padding the batches\n",
    "    compute_metrics=calculate_accuracy,  # Function to compute evaluation metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "763191b0-6636-4524-8c1c-6b8e36bdc751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5470' max='5470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5470/5470 2:14:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.661444</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.585200</td>\n",
       "      <td>0.566611</td>\n",
       "      <td>0.691467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.565562</td>\n",
       "      <td>0.705867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.554272</td>\n",
       "      <td>0.710133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.523800</td>\n",
       "      <td>0.530185</td>\n",
       "      <td>0.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.523800</td>\n",
       "      <td>0.536210</td>\n",
       "      <td>0.723467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0.508826</td>\n",
       "      <td>0.741333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.534256</td>\n",
       "      <td>0.731733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.530395</td>\n",
       "      <td>0.733867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.526986</td>\n",
       "      <td>0.734933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5470, training_loss=0.540597933084045, metrics={'train_runtime': 8051.4765, 'train_samples_per_second': 21.735, 'train_steps_per_second': 0.679, 'total_flos': 1.1540938752e+16, 'train_loss': 0.540597933084045, 'epoch': 10.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e4a8a-38f3-4333-b5b0-ab26017eb525",
   "metadata": {},
   "source": [
    "### Training Summary\n",
    "\n",
    "**Global Steps:** 5470  \n",
    "**Total Training Loss:** 0.540597933084045\n",
    "\n",
    "**Metrics:**\n",
    "- **Training Runtime:** 8051.4765 seconds\n",
    "- **Training Samples Per Second:** 21.735\n",
    "- **Training Steps Per Second:** 0.679\n",
    "- **Total FLOPs:** 1.1540938752e+16\n",
    "\n",
    "The training process indicates a consistent improvement in validation accuracy and a steady decrease in both training and validation loss over the epochs, demonstrating effective learning and generalization.\n",
    "lization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7bededa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine tuned PEFT model\n",
    "peft_model.save_pretrained(\"gpt-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"gpt-lora\", num_labels=NUM_LABELS, ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's pad token id to match the tokenizer's pad token id\n",
    "lora_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments for the HuggingFace Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/sentiment_analysis\",  # Directory to save model outputs\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=32,  # Batch size for training per device\n",
    "    per_device_eval_batch_size=32,  # Batch size for evaluation per device\n",
    "    num_train_epochs=10,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    evaluation_strategy=\"epoch\",  # Evaluation is performed at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for fine-tuning the model\n",
    "finetuned_trainer = Trainer(\n",
    "    model=lora_model,  # The fine-tuned PEFT model\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # Evaluation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer for encoding the data\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Data collator for padding the batches\n",
    "    compute_metrics=calculate_accuracy,  # Function to compute evaluation metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='118' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [118/118 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for the fine-tuned model: {'eval_loss': 0.5088258385658264, 'eval_accuracy': 0.7413333333333333, 'eval_runtime': 31.4576, 'eval_samples_per_second': 119.208, 'eval_steps_per_second': 3.751}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on the validation set\n",
    "finetuned_results = finetuned_trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results for the fine-tuned model\n",
    "print(\"Evaluation results for the fine-tuned model:\", finetuned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72ee6346-ed26-49f3-ac09-27bf92616733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference on a single example\n",
    "def perform_inference(text):\n",
    "    # Preprocess the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the same device as the model\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted class\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Map the predicted class to the corresponding label\n",
    "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    predicted_label = id2label[predicted_class]\n",
    "\n",
    "    return predicted_label\n",
    "rn predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05632f6a-22c2-49e0-96c3-4dabd87f4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for inference\n",
    "sample_text = \"The European Union includes how many countries?\"\n",
    "sample_text = \"Interesting facts about Egypt ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cefa1850-c868-46bd-8acd-f2e6cb95ad5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on the sample text\n",
    "predicted_label = perform_inference(sample_text)\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329d68f-1a37-4e63-8561-fe3816b57681",
   "metadata": {},
   "source": [
    "## Advanced Option : Applying quantization-aware training (QAT)\n",
    "\n",
    "Quantization can help in reducing the model size and speeding up inference. Below is an advanced script that incorporates quantization for a LoRA fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "925ca0cf-176d-46cd-a6b5-83d4be606d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "peft_model.save_pretrained(\"./lora_model_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afc97336-43f2-4a22-89fb-c0982f93a683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model for quantization\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./lora_model_output\")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3483e2e1-ccb2-4b4a-a02c-bbb9d761317d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): lora.Linear(\n",
       "            (base_layer): Conv1D()\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (c_proj): lora.Linear(\n",
       "            (base_layer): Conv1D()\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): lora.Linear(\n",
       "            (base_layer): Conv1D()\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): ModulesToSaveWrapper(\n",
       "    (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "    (modules_to_save): ModuleDict(\n",
       "      (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03530b1d-3fe9-401f-929f-5304a620599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mniti\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:312: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
     ]
    }
   ],
   "source": [
    "# Prepare for quantization\n",
    "model = torch.quantization.prepare_qat(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd765f9c-2a35-409c-b9e2-192a762cebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model with quantization-aware training\n",
    "training_args.num_train_epochs = 1  # Fine-tune for one more epoch with QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d33f8bf-d151-48c6-8da9-3f16666e1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,  # The PEFT model for training.\n",
    "    args=training_args,  # Training arguments, defined previously.\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Tokenized training dataset.\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # Tokenized validation dataset.\n",
    "    tokenizer=tokenizer,  # The tokenizer used for encoding the data.\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Data collator for padding sequences.\n",
    "    compute_metrics=calculate_accuracy,  # Function to compute evaluation metrics.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "447fcbf5-6b23-4115-b24b-f1688994f377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='547' max='547' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [547/547 33:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.533796</td>\n",
       "      <td>0.737067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=547, training_loss=0.5042334492071455, metrics={'train_runtime': 1993.4159, 'train_samples_per_second': 8.779, 'train_steps_per_second': 0.274, 'total_flos': 1154093875200000.0, 'train_loss': 0.5042334492071455, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with QAT\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48ddafc4-dfb5-4168-862a-85a110b6a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to quantized model\n",
    "quantized_model = torch.quantization.convert(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "acaa8589-7d06-4055-b574-758ea8b74d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "torch.save(quantized_model.state_dict(), \"./quantized_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c5a2843-b64e-4812-ba92-7d85d9ac99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference on a single example using the quantized model\n",
    "def perform_inference(text, quantized_model):\n",
    "    # Preprocess the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the same device as the model\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = quantized_model(**inputs)\n",
    "\n",
    "    # Get the predicted class\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Map the predicted class to the corresponding label\n",
    "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    predicted_label = id2label[predicted_class]\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43c5edf8-369e-4c7a-9feb-20ad814efd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): lora.Linear(\n",
       "            (base_layer): Conv1D()\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (c_proj): lora.Linear(\n",
       "            (base_layer): Conv1D()\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): lora.Linear(\n",
       "            (base_layer): Conv1D()\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): ModulesToSaveWrapper(\n",
       "    (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "    (modules_to_save): ModuleDict(\n",
       "      (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the quantized model for inference\n",
    "quantized_model.load_state_dict(torch.load(\"./quantized_model.pth\"))\n",
    "quantized_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "662bd4f1-1b87-4ce7-a9d9-e873346af912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for inference\n",
    "sample_text = \"The European Union includes how many countries?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "440e398b-81e8-4e64-9c0a-a1c705c9e585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on the sample text\n",
    "predicted_label = perform_inference(sample_text, quantized_model)\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57636b-013d-47ab-bee8-679fdc610da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
